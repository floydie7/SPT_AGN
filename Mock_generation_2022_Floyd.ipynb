{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Mock Generation Summer 2022\n",
    "Author: Benjamin Floyd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RNG seed: 123\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import emcee\n",
    "import corner\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "from astropy.table import Table, vstack, join, unique\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "from scipy import stats\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "from schwimmbad import MultiPool\n",
    "import astropy.units as u\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "cosmo = FlatLambdaCDM(H0=70, Om0=0.3)\n",
    "\n",
    "# Set up rng\n",
    "seed = 123\n",
    "rng = np.random.default_rng(seed)\n",
    "print(f'Using RNG seed: {seed}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mock generation functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def poisson_point_process(rate, dx, dy=None, lower_dx=0, lower_dy=0):\n",
    "    \"\"\"\n",
    "    Uses a spatial Poisson point process to generate AGN candidate coordinates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rate : float\n",
    "        The model rate used in the Poisson distribution to determine the number of points being placed.\n",
    "    dx, dy : int, Optional\n",
    "        Upper bound on x- and y-axes respectively. If only `dx` is provided then `dy` = `dx`.\n",
    "    lower_dx, lower_dy : int, Optional\n",
    "        Lower bound on x- and y-axes respectively. If not provided, a default of 0 will be used\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    coord : np.ndarray\n",
    "        Numpy array of (x, y) coordinates of AGN candidates\n",
    "    \"\"\"\n",
    "\n",
    "    if dy is None:\n",
    "        dy = dx\n",
    "\n",
    "    # Draw from Poisson distribution to determine how many points we will place.\n",
    "    p = stats.poisson(rate * np.abs(dx - lower_dx) * np.abs(dy - lower_dy)).rvs(random_state=rng)\n",
    "\n",
    "    # Drop `p` points with uniform x and y coordinates\n",
    "    x = rng.uniform(lower_dx, dx, size=p)\n",
    "    y = rng.uniform(lower_dy, dy, size=p)\n",
    "\n",
    "    # Combine the x and y coordinates.\n",
    "    coord = np.vstack((x, y))\n",
    "\n",
    "    return coord"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def weighted_poisson_process(target_rate, weighted_data, selection_threshold, tol, area, dx, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates a poisson point process that incorporates objects with weights assigned to them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target_rate : float\n",
    "        The targeted Poisson rate. This will be scaled up to drop the initial sample and will be sampled down to match within the given tolerance.\n",
    "    weighted_data : array-like, Table, pd.DataFrame\n",
    "        Data structure containing weights and any other associated data.\n",
    "    selection_threshold : float\n",
    "        Selection color threshold corresponding to th 90% purity level being used for this realization.\n",
    "    tol : float\n",
    "        Tolerance required to match the sampled weighted rate to the targeted Poisson rate.\n",
    "    area : float\n",
    "        Total value of allowable area for objects to be placed.\n",
    "    dx : int\n",
    "        Upper bound of x-axis of bounding box used to place points within. If only `dx` is given, then the bounding box is assumed to be square\n",
    "    **kwargs\n",
    "        Keyword arguments to be passed to `poisson_point_process`.\n",
    "    Returns\n",
    "    -------\n",
    "    catalog : Table\n",
    "        Output catalog with positions of objects as well as any associated data corresponding to the weights used.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initial sampling will be well oversampled\n",
    "    coords = poisson_point_process(target_rate * 1000, dx=dx, **kwargs)\n",
    "\n",
    "    # Populate a DataFrame with the objects\n",
    "    if isinstance(weighted_data, pd.DataFrame):\n",
    "        weighted_data_df = weighted_data\n",
    "    elif isinstance(weighted_data, Table):\n",
    "        weighted_data_df = weighted_data.to_pandas()\n",
    "    else:\n",
    "        weighted_data_df = pd.DataFrame(data=weighted_data)\n",
    "    weighted_data_df = weighted_data_df.sample(n=coords.shape[-1], replace=True, random_state=rng)\n",
    "    weighted_data_table = Table.from_pandas(weighted_data_df)\n",
    "    weighted_data_table['x'] = coords[0]\n",
    "    weighted_data_table['y'] = coords[1]\n",
    "    weighted_data_table['radial_dist'] = np.sqrt((coords[0] - (dx / 2)) ** 2 + (coords[1] - (dx / 2)) ** 2)\n",
    "\n",
    "    # For now, filter out any object that is outside the range\n",
    "    weighted_data_table = weighted_data_table[weighted_data_table['radial_dist'] <= 2.5]\n",
    "\n",
    "    # Iterate through the weighted data and select objects until we overshoot our target rate\n",
    "    output_data, weights = [], []\n",
    "    for i, mu in enumerate(weighted_data_table[f'SELECTION_MEMBERSHIP_{selection_threshold:.2f}']):\n",
    "        proposal = [*weights, mu]\n",
    "        if np.sum(proposal) / area > target_rate:\n",
    "            output_data.append(weighted_data_table[i])\n",
    "            weights.append(mu)\n",
    "            break\n",
    "        else:\n",
    "            output_data.append(weighted_data_table[i])\n",
    "            weights.append(mu)\n",
    "\n",
    "    # Check the errors for the last and penultimate weights used\n",
    "    last_err = np.abs(np.sum(weights) / area - target_rate)\n",
    "    prev_err = np.abs(np.sum(weights[:-1]) / area - target_rate)\n",
    "\n",
    "    # Drop the last object if the error is smaller without it.\n",
    "    if last_err >= prev_err:\n",
    "        output_data = output_data[:-1]\n",
    "\n",
    "    # Concatenate the output data into a DataFrame and recast as an Astropy table.\n",
    "    # output_data_df = pd.concat(output_data)\n",
    "    output_data_table = vstack(output_data)\n",
    "\n",
    "    # Select only the relevant columns and rename columns for standardization\n",
    "    output_data_table = output_data_table[\n",
    "        'x', 'y', 'radial_dist', 'REDSHIFT', 'COMPLETENESS_CORRECTION', f'SELECTION_MEMBERSHIP_{selection_threshold:.2f}']\n",
    "    output_data_table.rename_columns(['REDSHIFT', f'SELECTION_MEMBERSHIP_{selection_threshold:.2f}'],\n",
    "                                     ['galaxy_redshift', 'SELECTION_MEMBERSHIP'])\n",
    "\n",
    "    return output_data_table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def model_rate(params, radial_dist, cluster_id):\n",
    "    \"\"\"\n",
    "    Our generating model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : tuple of floats\n",
    "        Tuple of parameters.\n",
    "    radial_dist : array-like\n",
    "        A vector of radii of objects relative to the cluster center\n",
    "    cluster_id : int or str\n",
    "        Used to select correct background prior\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : np.ndarray\n",
    "        A surface density profile of objects as a function of radius.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the parameters\n",
    "    theta, beta, c0 = params\n",
    "    rc = rc_true\n",
    "\n",
    "    # In mock generation, we need to be able to skip adding the background surface density redshift relation.\n",
    "    if cluster_id == -1:\n",
    "        cz = 0.\n",
    "    else:\n",
    "        z = master_catalog['REDSHIFT'][master_catalog['SPT_ID'] == cluster_id][0]\n",
    "        cz = c0 + delta_c(z) * num_clusters\n",
    "\n",
    "    # Our amplitude will eventually be more complicated\n",
    "    a = theta\n",
    "\n",
    "    # Our model rate is an amplitude of cluster-specific trends with a radial dependence with a constant background rate.\n",
    "    model = a * (1 + (radial_dist / rc) ** 2) ** (-1.5 * beta + 0.5) + cz\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def generate_mock_cluster(cluster_catalog: Table, color_threshold: float, c_true: float, c_true_err: float) -> Table:\n",
    "    # Background Catalog\n",
    "    bkg_cat = weighted_poisson_process(c_true, sdwfs_agn, color_threshold, tol=c_true_err, area=image_width*image_width, dx=image_width)\n",
    "\n",
    "    # Add flag to background objects\n",
    "    bkg_cat['CLUSTER_AGN'] = np.full_like(bkg_cat['x'], False)\n",
    "\n",
    "    # Cluster Catalog\n",
    "    # Set an array of radii to generate model rates upon\n",
    "    r_grid = np.linspace(0., 5., num=100)\n",
    "\n",
    "    # Find the maximum rate of our model to use to for homogeneous Poisson process (Using c = 0.0 for a cluster-only model)\n",
    "    max_rate = np.max(model_rate(params=(a_true, beta_true, 0.0), radial_dist=r_grid, cluster_id=-1))\n",
    "\n",
    "    # For the cluster, we need to select only objects within a redshift range of the cluster redshift.\n",
    "    sdwfs_agn_at_z = sdwfs_agn[np.abs(sdwfs_agn['REDSHIFT'] - cluster_catalog['REDSHIFT']) <= cluster_catalog['REDSHIFT_UNC']]\n",
    "\n",
    "    # Generate the homogenous Poisson process (Again, this will need to be done on pixel units in the future.)\n",
    "    cl_cat = weighted_poisson_process(max_rate, sdwfs_agn_at_z, color_threshold, tol=c_true_err, area=image_width*image_width, dx=image_width)\n",
    "\n",
    "    # Compute model rates at each candidate position\n",
    "    rate_at_radius = model_rate(params=(a_true, beta_true, 0.), radial_dist=cl_cat['radial_dist'], cluster_id=-1)\n",
    "\n",
    "    # Perform rejection sampling\n",
    "    prob_reject = rate_at_radius / max_rate\n",
    "    alpha = rng.uniform(0., 1., size=len(rate_at_radius))\n",
    "    cl_cat = cl_cat[prob_reject >= alpha]\n",
    "\n",
    "    # Add flag to cluster objects\n",
    "    cl_cat['CLUSTER_AGN'] = np.full_like(cl_cat['x'], True)\n",
    "\n",
    "    # Merge the catalogs\n",
    "    los_cat = vstack([cl_cat, bkg_cat])\n",
    "\n",
    "    # Add a cluster ID\n",
    "    los_cat['SPT_ID'] = cluster_catalog['SPT_ID']\n",
    "\n",
    "    # Add a column for the cluster redshift. (Using generic \"REDSHIFT\" to match real catalog expectation)\n",
    "    los_cat['REDSHIFT'] = cluster_catalog['REDSHIFT']\n",
    "\n",
    "    return los_cat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bayesian model functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def lnlike(params: tuple[float, ...]):\n",
    "    # Compute the likelihood value for each cluster\n",
    "    cluster_like = []\n",
    "    for cluster in master_catalog.group_by('SPT_ID').groups:\n",
    "        cluster_id = cluster['SPT_ID'][0]\n",
    "\n",
    "        # Get the selection membership of each object\n",
    "        mu_agn = cluster['SELECTION_MEMBERSHIP']\n",
    "\n",
    "        # Compute the model rate at locations of the AGN.\n",
    "        ri = cluster['radial_dist']\n",
    "        ni = model_rate(params, ri, cluster_id)\n",
    "\n",
    "        # Compute the ideal model rate at continuous locations\n",
    "        rall = np.linspace(0., 2.5, num=10_000)\n",
    "        nall = model_rate(params, rall, cluster_id)\n",
    "\n",
    "        # We use a Poisson likelihood function\n",
    "        ln_like_func = np.sum(np.log(ni * ri) * mu_agn) - np.trapz(nall * 2 * np.pi * rall, rall)\n",
    "        cluster_like.append(ln_like_func)\n",
    "\n",
    "    # Compute the total likelihood value\n",
    "    total_ln_like = np.sum(cluster_like)\n",
    "    return total_ln_like"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def lnprior(params: tuple[float, ...]):\n",
    "    # Extract the parameters\n",
    "    theta, beta, c0 = params\n",
    "    rc = rc_true\n",
    "\n",
    "    cluster_prior = []\n",
    "    for cluster in master_catalog.group_by('SPT_ID').groups:\n",
    "        # Get the cluster redshift to set the background hyperparameters\n",
    "        z = cluster['REDSHIFT'][0]\n",
    "        h_c = agn_prior_surf_den(z) * num_clusters\n",
    "        h_c_err = agn_prior_surf_den_err(z) * num_clusters\n",
    "\n",
    "        # Shift background parameter to redshift-dependent value.\n",
    "        cz = c0 + delta_c(z) * num_clusters\n",
    "\n",
    "        # Define parameter ranges\n",
    "        if (0. <= theta <= np.inf and\n",
    "            -3. <= beta <= 3. and\n",
    "            0. <= rc <= 0.5 and\n",
    "            0. <= cz <= np.inf):\n",
    "            theta_lnprior = 0.\n",
    "            beta_lnprior = 0.\n",
    "            rc_lnprior = 0.\n",
    "            c_lnprior = -0.5 * np.sum((cz - h_c) ** 2 / h_c_err ** 2)\n",
    "        else:\n",
    "            theta_lnprior = -np.inf\n",
    "            beta_lnprior = -np.inf\n",
    "            rc_lnprior = -np.inf\n",
    "            c_lnprior = -np.inf\n",
    "        ln_prior_prob = theta_lnprior + beta_lnprior + rc_lnprior + c_lnprior\n",
    "        cluster_prior.append(ln_prior_prob)\n",
    "\n",
    "    total_lnprior = np.sum(cluster_prior)\n",
    "    return total_lnprior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def lnprob(params: tuple[float, ...]):\n",
    "    # Evaluate log-prior and test if we are within bounds\n",
    "    lp = lnprior(params)\n",
    "\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "\n",
    "    return lnlike(params) + lp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate mock catalog"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Select out a number of clusters to use as examples\n",
    "n_cl = 2\n",
    "\n",
    "# Read in the SDWFS IRAGN catalog for use later\n",
    "sdwfs_agn = Table.read('Data_Repository/Project_Data/SPT-IRAGN/Output/SDWFS_cutout_IRAGN.fits')\n",
    "\n",
    "# Read in the SPT cluster catalog. We will use real data to source our mock cluster properties.\n",
    "Bocquet = Table.read(f'Data_Repository/Catalogs/SPT/SPT_catalogs/2500d_cluster_sample_Bocquet18.fits')\n",
    "\n",
    "# For the 20 common clusters between SPT-SZ 2500d and SPTpol 100d surveys we want to update the cluster information from\n",
    "# the more recent survey. Thus, we will merge the SPT-SZ and SPTpol catalogs together.\n",
    "Huang = Table.read(f'Data_Repository/Catalogs/SPT/SPT_catalogs/sptpol100d_catalog_huang19.fits')\n",
    "\n",
    "# First we need to rename several columns in the SPTpol 100d catalog to match the format of the SPT-SZ catalog\n",
    "Huang.rename_columns(['Dec', 'xi', 'theta_core', 'redshift', 'redshift_unc'],\n",
    "                     ['DEC', 'XI', 'THETA_CORE', 'REDSHIFT', 'REDSHIFT_UNC'])\n",
    "\n",
    "# Now, merge the two catalogs\n",
    "SPTcl = join(Bocquet, Huang, join_type='outer')\n",
    "SPTcl.sort(keys=['SPT_ID', 'field'])  # Sub-sorting by 'field' puts Huang entries first\n",
    "SPTcl = unique(SPTcl, keys='SPT_ID', keep='first')  # Keeping Huang entries over Bocquet\n",
    "SPTcl.sort(keys='SPT_ID')  # Resort by ID.\n",
    "\n",
    "# Convert masses to [Msun] rather than [Msun/1e14]\n",
    "SPTcl['M500'] *= 1e14\n",
    "SPTcl['M500_uerr'] *= 1e14\n",
    "SPTcl['M500_lerr'] *= 1e14\n",
    "\n",
    "# Remove any unconfirmed clusters\n",
    "SPTcl = SPTcl[SPTcl['M500'] > 0.0]\n",
    "\n",
    "# For our masks, we will co-op the masks for the real clusters.\n",
    "masks_files = [*glob.glob(f'Data_Repository/Project_Data/SPT-IRAGN/Masks/SPT-SZ_2500d/*.fits'),\n",
    "               *glob.glob(f'Data_Repository/Project_Data/SPT-IRAGN/Masks/SPTpol_100d/*.fits')]\n",
    "\n",
    "# Make sure all the masks have matches in the catalog\n",
    "masks_files = [f for f in masks_files if re.search(r'SPT-CLJ\\d+-\\d+', f).group(0) in SPTcl['SPT_ID']]\n",
    "\n",
    "# Select a number of masks at random, sorted to match the order in `full_spt_catalog`.\n",
    "masks_bank = sorted([masks_files[i] for i in rng.choice(n_cl, size=n_cl)],\n",
    "                    key=lambda x: re.search(r'SPT-CLJ\\d+-\\d+', x).group(0))\n",
    "\n",
    "# Find the corresponding cluster IDs in the SPT catalog that match the masks we chose\n",
    "spt_catalog_ids = [re.search(r'SPT-CLJ\\d+-\\d+', mask_name).group(0) for mask_name in masks_bank]\n",
    "spt_catalog_mask = [np.where(SPTcl['SPT_ID'] == spt_id)[0][0] for spt_id in spt_catalog_ids]\n",
    "selected_clusters = SPTcl['SPT_ID', 'RA', 'DEC', 'M500', 'REDSHIFT', 'REDSHIFT_UNC', 'THETA_CORE', 'XI', 'field'][spt_catalog_mask]\n",
    "\n",
    "# We'll need the r500 radius for each cluster too.\n",
    "selected_clusters['R500'] = (3 * selected_clusters['M500'] * u.Msun /\n",
    "                             (4 * np.pi * 500 *\n",
    "                              cosmo.critical_density(selected_clusters['REDSHIFT']).to(u.Msun / u.Mpc ** 3))) ** (1 / 3)\n",
    "\n",
    "# Create cluster names\n",
    "name_bank = [f'SPT_Mock_{i:03d}' for i in range(n_cl)]\n",
    "\n",
    "# Combine our data into a catalog\n",
    "SPT_data = selected_clusters.copy()\n",
    "SPT_data.rename_columns(['SPT_ID', 'RA', 'DEC'], ['orig_SPT_ID', 'SZ_RA', 'SZ_DEC'])\n",
    "SPT_data['SPT_ID'] = name_bank\n",
    "SPT_data['MASK_NAME'] = masks_bank"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Read in the purity and surface density files\n",
    "with (open('Data_Repository/Project_Data/SPT-IRAGN/SDWFS_background/SDWFS_purity_color.json', 'r') as f,\n",
    "      open('Data_Repository/Project_Data/SPT-IRAGN/SDWFS_background/SDWFS_background_prior_distributions.json', 'r') as g):\n",
    "    sdwfs_purity_data = json.load(f)\n",
    "    sdwfs_prior_data = json.load(g)\n",
    "z_bins = sdwfs_purity_data['redshift_bins'][:-1]\n",
    "threshold_bins = sdwfs_prior_data['color_thresholds'][:-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Set up interpolators\n",
    "agn_purity_color = interp1d(z_bins, sdwfs_purity_data['purity_90_colors'], kind='previous')\n",
    "agn_surf_den = interp1d(threshold_bins, sdwfs_prior_data['agn_surf_den'], kind='previous')\n",
    "agn_surf_den_err = interp1d(threshold_bins, sdwfs_prior_data['agn_surf_den_err'], kind='previous')\n",
    "\n",
    "# For convenience, set up the function compositions\n",
    "def agn_prior_surf_den(redshift: float) -> float:\n",
    "    return agn_surf_den(agn_purity_color(redshift))\n",
    "\n",
    "def agn_prior_surf_den_err(redshift: float) -> float:\n",
    "    return agn_surf_den_err(agn_purity_color(redshift))\n",
    "\n",
    "# Set up an interpolation for the AGN surface density relative to the reference surface density at z = 0\n",
    "delta_c = interp1d(z_bins, agn_prior_surf_den(z_bins) - agn_prior_surf_den(0.), kind='previous')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input parameters: a_true = 125.0, beta_true = 1.0, rc_true = 0.1, c0_true = 7.905 (c_truths = array([19.86808173, 19.86808173]))\n"
     ]
    }
   ],
   "source": [
    "cluster_redshifts = SPT_data['REDSHIFT']\n",
    "redshift_uncert = SPT_data['REDSHIFT_UNC']\n",
    "\n",
    "# For a cluster at z = 0.6, the color threshold will be [3.6] - [4.5] = 0.61\n",
    "color_thresholds = [agn_purity_color(z) for z in cluster_redshifts]\n",
    "\n",
    "# For now, we will set a fixed window size of 5'x5'. This will need to be generalized to use the mask files' WCS.\n",
    "image_width = 5.\n",
    "\n",
    "# We'll boost the number of objects in our sample by duplicating this cluster by a factor.\n",
    "num_clusters = 50\n",
    "\n",
    "# We will set our input (true) parameters to be an arbitrary value for cluster and using an approximation of the expected background surface density using our color threshold.\n",
    "a_true = 2.5\n",
    "beta_true = 1.0\n",
    "rc_true = 0.1\n",
    "c0_true = agn_prior_surf_den(0.)\n",
    "c_truths = np.array([agn_prior_surf_den(z) for z in cluster_redshifts])\n",
    "c_err_truths = np.array([agn_surf_den_err(z) for z in cluster_redshifts])\n",
    "\n",
    "# We will amplify the true parameters by the number of clusters in the sample.\n",
    "a_true *= num_clusters\n",
    "c0_true *= num_clusters\n",
    "c_truths *= num_clusters\n",
    "c_err_truths *= num_clusters\n",
    "print(f'Input parameters: {a_true = }, {beta_true = }, {rc_true = }, {c0_true = :.3f} ({c_truths = })')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run cluster realization pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m cluster_cats \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m cluster_catalog, cluster_color_threshold, bkg_rate_true, bkg_rate_err_true \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(SPT_data, color_thresholds, c_truths, c_err_truths):\n\u001B[0;32m----> 3\u001B[0m     cat \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_mock_cluster\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcluster_catalog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcluster_color_threshold\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbkg_rate_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbkg_rate_err_true\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m     cluster_cats\u001B[38;5;241m.\u001B[39mappend(cat)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;66;03m# Show plot of combined line-of-sight positions\u001B[39;00m\n",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36mgenerate_mock_cluster\u001B[0;34m(cluster_catalog, color_threshold, c_true, c_true_err)\u001B[0m\n\u001B[1;32m     16\u001B[0m sdwfs_agn_at_z \u001B[38;5;241m=\u001B[39m sdwfs_agn[np\u001B[38;5;241m.\u001B[39mabs(sdwfs_agn[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mREDSHIFT\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m-\u001B[39m cluster_catalog[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mREDSHIFT\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m cluster_catalog[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mREDSHIFT_UNC\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Generate the homogenous Poisson process (Again, this will need to be done on pixel units in the future.)\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m cl_cat \u001B[38;5;241m=\u001B[39m \u001B[43mweighted_poisson_process\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msdwfs_agn_at_z\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolor_threshold\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mc_true_err\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marea\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_width\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mimage_width\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_width\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Compute model rates at each candidate position\u001B[39;00m\n\u001B[1;32m     22\u001B[0m rate_at_radius \u001B[38;5;241m=\u001B[39m model_rate(params\u001B[38;5;241m=\u001B[39m(a_true, beta_true, \u001B[38;5;241m0.\u001B[39m), radial_dist\u001B[38;5;241m=\u001B[39mcl_cat[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mradial_dist\u001B[39m\u001B[38;5;124m'\u001B[39m], cluster_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "Input \u001B[0;32mIn [3]\u001B[0m, in \u001B[0;36mweighted_poisson_process\u001B[0;34m(target_rate, weighted_data, selection_threshold, tol, area, dx, **kwargs)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, mu \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(weighted_data_table[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSELECTION_MEMBERSHIP_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mselection_threshold\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m]):\n\u001B[1;32m     49\u001B[0m     proposal \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m*\u001B[39mweights, mu]\n\u001B[0;32m---> 50\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mproposal\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m/\u001B[39m area \u001B[38;5;241m>\u001B[39m target_rate:\n\u001B[1;32m     51\u001B[0m         output_data\u001B[38;5;241m.\u001B[39mappend(weighted_data_table[i])\n\u001B[1;32m     52\u001B[0m         weights\u001B[38;5;241m.\u001B[39mappend(mu)\n",
      "File \u001B[0;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36msum\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m~/.conda/envs/astro/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2296\u001B[0m, in \u001B[0;36msum\u001B[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m   2293\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[1;32m   2294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\u001B[0;32m-> 2296\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapreduction\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2297\u001B[0m \u001B[43m                      \u001B[49m\u001B[43minitial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/astro/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86\u001B[0m, in \u001B[0;36m_wrapreduction\u001B[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001B[0m\n\u001B[1;32m     83\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     84\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m reduction(axis\u001B[38;5;241m=\u001B[39maxis, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpasskwargs)\n\u001B[0;32m---> 86\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mufunc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpasskwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "cluster_cats = []\n",
    "for cluster_catalog, cluster_color_threshold, bkg_rate_true, bkg_rate_err_true in zip(SPT_data, color_thresholds, c_truths, c_err_truths):\n",
    "    cat = generate_mock_cluster(cluster_catalog, cluster_color_threshold, bkg_rate_true, bkg_rate_err_true)\n",
    "    cluster_cats.append(cat)\n",
    "\n",
    "    # Show plot of combined line-of-sight positions\n",
    "    cluster_objects = cat[cat['CLUSTER_AGN'].astype(bool)]\n",
    "    background_objects = cat[~cat['CLUSTER_AGN'].astype(bool)]\n",
    "    _, ax = plt.subplots()\n",
    "    ax.scatter(background_objects['x'], background_objects['y'], edgecolors='blue', facecolors='none', alpha=0.4, label='Background')\n",
    "    ax.scatter(cluster_objects['x'], cluster_objects['y'], edgecolors='red', facecolors='red', alpha=0.6, label='Cluster')\n",
    "    ax.legend()\n",
    "    ax.set(title=f'{cluster_catalog[\"SPT_ID\"]} at z = {cluster_catalog[\"REDSHIFT\"]:.2f}', xlabel='x [arcmin]', ylabel='y [arcmin]', xlim=[0, image_width], ylim=[0, image_width], aspect=1)\n",
    "    plt.show()\n",
    "\n",
    "# Combine all catalogs\n",
    "master_catalog = vstack(cluster_cats)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apply Bayesian model to refit data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set up walkers\n",
    "ndim = 3\n",
    "nwalkers = 50\n",
    "nsteps = 5000\n",
    "\n",
    "# Initialize walker positions\n",
    "# pos0 = np.array([rng.normal(a_true, 1e-4, size=nwalkers), rng.normal(beta_true, 1e-4, size=nwalkers), rng.normal(rc_true, 1e-4, size=nwalkers), rng.normal(c_true, 1e-4, size=nwalkers)]).T\n",
    "pos0 = np.array([rng.normal(a_true, 1e-4, size=nwalkers), rng.normal(beta_true, 1e-4, size=nwalkers), rng.normal(c0_true, 1e-4, size=nwalkers)]).T\n",
    "\n",
    "with MultiPool() as pool:\n",
    "    sampler = emcee.EnsembleSampler(nwalkers=nwalkers, ndim=ndim, log_prob_fn=lnprob, pool=pool)\n",
    "    sampler.run_mcmc(pos0, nsteps=nsteps, progress=True)\n",
    "\n",
    "print(f'Mean autocorrelation time: {(mean_tau := np.mean(sampler.get_autocorr_time())):.2f} steps\\n', f'Mean acceptance fraction: {np.mean(sampler.acceptance_fraction):.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot chains\n",
    "samples = sampler.get_chain()\n",
    "labels = ['A', r'$\\beta$', r'$C_0$']\n",
    "truths = [a_true, beta_true, c0_true]\n",
    "_, axes = plt.subplots(nrows=ndim, figsize=(10, 7), sharex='col')\n",
    "for i, (ax, label, truth) in enumerate(zip(axes.flatten(), labels, truths)):\n",
    "    ax.plot(samples[:, :, i], 'k', alpha=0.3)\n",
    "    ax.axhline(y=truth, c='b')\n",
    "    ax.set(ylabel=label, xlim=[0, len(samples)])\n",
    "axes[-1].set(xlabel='Steps')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot posterior\n",
    "flat_samples = sampler.get_chain(discard=int(3*mean_tau), flat=True)\n",
    "fig = corner.corner(flat_samples, labels=labels, truths=truths, show_titles=True, quantiles=[0.16, 0.5, 0.84])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}